"""
Report generation for methodological transparency.

This module generates publication-ready markdown reports that narrate
the entire harmonization process in clear, academic language.
"""

from pathlib import Path
from typing import Dict, Any, Optional
import pandas as pd

from autogbd.core.provenance import ProvenanceTracker


class ReportGenerator:
    """
    Generator for harmonization methodology reports.

    Creates publication-ready markdown reports that document every
    transformation applied during the harmonization process.
    """

    def __init__(self):
        """Initialize the report generator."""

    def generate(
        self,
        data: pd.DataFrame,
        quality_results: Optional[Dict[str, Any]],
        provenance: ProvenanceTracker,
        output_path: Path,
    ) -> None:
        """
        Generate a harmonization methodology report.

        Parameters
        ----------
        data : pd.DataFrame
            Final harmonized data.
        quality_results : dict, optional
            Results from quality checks.
        provenance : ProvenanceTracker
            Provenance tracker with complete audit trail.
        output_path : Path
            Path where the report should be saved.
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        report_sections = []

        # Title and introduction
        report_sections.append(self._generate_title_section(provenance))

        # Executive summary
        report_sections.append(self._generate_summary_section(data, quality_results, provenance))

        # Data loading section
        report_sections.append(self._generate_data_loading_section(provenance))

        # Data cleaning section
        report_sections.append(self._generate_cleaning_section(provenance))

        # Mapping section
        report_sections.append(self._generate_mapping_section(provenance))

        # Quality assessment section
        report_sections.append(self._generate_quality_section(quality_results, provenance))

        # Final output section
        report_sections.append(self._generate_output_section(data, provenance))

        # Appendices
        report_sections.append(self._generate_appendices(provenance))

        # Write report
        report_content = "\n\n".join(report_sections)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

    def _generate_title_section(self, provenance: ProvenanceTracker) -> str:
        """Generate report title and header."""
        return f"""# Data Harmonization Methodology Report

**Run ID:** {provenance.run_id}  
**Date:** {provenance.start_time.strftime('%Y-%m-%d %H:%M:%S')}  
**Generated by:** AutoGBD Framework v0.1.0

---

## Abstract

This report documents the complete methodology and transformations applied during the harmonization of health data to the Global Burden of Disease (GBD) cause list standards. All transformations are fully reproducible and documented in the accompanying provenance log.
"""

    def _generate_summary_section(
        self,
        data: pd.DataFrame,
        quality_results: Optional[Dict[str, Any]],
        provenance: ProvenanceTracker,
    ) -> str:
        """Generate executive summary."""
        summary = provenance.get_summary()

        quality_score = (
            quality_results.get("quality_score", "N/A")
            if quality_results
            else "N/A"
        )

        duration_hours = provenance.to_dict().get("duration_seconds", 0) / 3600

        return f"""## Executive Summary

This harmonization process transformed raw health data into an analysis-ready format aligned with GBD 2021 cause list standards.

**Summary Statistics:**
- **Total Rows (Final):** {len(data):,}
- **Total Columns:** {len(data.columns)}
- **Quality Score:** {quality_score}
- **Processing Duration:** {duration_hours:.2f} hours
- **Total Transformations:** {summary.get('total_entries', 0)}

**Pipeline Stages:**
{self._format_stage_summary(summary.get('steps', {}))}
"""

    def _format_stage_summary(self, steps: Dict[str, Any]) -> str:
        """Format stage summary for markdown."""
        lines = []
        for step_name, step_info in steps.items():
            lines.append(
                f"- **{step_name.title()}**: {step_info.get('entry_count', 0)} actions, "
                f"{step_info.get('total_rows_affected', 0):,} rows affected"
            )
        return "\n".join(lines) if lines else "- No stages recorded"

    def _generate_data_loading_section(self, provenance: ProvenanceTracker) -> str:
        """Generate data loading section."""
        io_entries = [e for e in provenance.entries if e.step == "io"]

        if not io_entries:
            return "## Data Loading\n\nNo data loading information available."

        load_entry = next((e for e in io_entries if e.action == "data_loaded"), None)

        if not load_entry:
            return "## Data Loading\n\nData loading completed (details in provenance log)."

        details = load_entry.details
        rows = details.get("rows", "Unknown")
        columns = details.get("columns", [])

        return f"""## Data Loading

A total of **{rows:,} rows** were loaded from the input file. The dataset contained the following columns:

{self._format_column_list(columns)}

**Input File:** {load_entry.file_used or 'Not specified'}
"""

    def _format_column_list(self, columns: list) -> str:
        """Format column list as markdown."""
        if not columns:
            return "- *No columns specified*"
        return "\n".join([f"- `{col}`" for col in columns])

    def _generate_cleaning_section(self, provenance: ProvenanceTracker) -> str:
        """Generate data cleaning section."""
        cleaning_entries = [e for e in provenance.entries if e.step == "cleaning"]

        if not cleaning_entries:
            return "## Data Cleaning\n\nNo data cleaning was performed."

        # Group by action
        actions = {}
        for entry in cleaning_entries:
            if entry.action not in actions:
                actions[entry.action] = []
            actions[entry.action].append(entry)

        sections = ["## Data Cleaning\n"]

        initial_entry = next(
            (e for e in cleaning_entries if e.action == "start_cleaning"), None
        )
        if initial_entry:
            rules_count = initial_entry.details.get("rules_enabled", 0)
            sections.append(f"A total of **{rules_count}** cleaning rules were applied.\n")

        # Document each cleaning action
        for action, entries in actions.items():
            if action in ["start_cleaning", "cleaning_complete"]:
                continue

            for entry in entries:
                rows_affected = entry.rows_affected or 0
                rule_name = entry.rule_name or action

                action_desc = action.replace("_", " ").title()
                sections.append(
                    f"### {action_desc}\n\n"
                    f"- **Rule:** {rule_name}\n"
                    f"- **Rows Affected:** {rows_affected:,}\n"
                )

                if entry.details:
                    details_str = ", ".join([f"{k}: {v}" for k, v in entry.details.items()])
                    if details_str:
                        sections.append(f"- **Details:** {details_str}\n")

        final_entry = next(
            (e for e in cleaning_entries if e.action == "cleaning_complete"), None
        )
        if final_entry:
            details = final_entry.details
            initial = details.get("initial_rows", 0)
            final = details.get("final_rows", 0)
            removed = details.get("rows_removed", 0)
            sections.append(
                f"\n**Cleaning Summary:**\n"
                f"- Initial rows: {initial:,}\n"
                f"- Final rows: {final:,}\n"
                f"- Rows removed: {removed:,}\n"
            )

        return "\n".join(sections)

    def _generate_mapping_section(self, provenance: ProvenanceTracker) -> str:
        """Generate cause mapping section."""
        mapping_entries = [e for e in provenance.entries if e.step == "mapping"]

        if not mapping_entries:
            return "## Cause Mapping\n\nNo cause mapping was performed."

        sections = ["## Cause Mapping\n"]

        # Get mapping completion entry
        complete_entry = next(
            (e for e in mapping_entries if e.action == "mapping_complete"), None
        )

        if complete_entry:
            details = complete_entry.details
            initial = details.get("initial_rows", 0)
            mapped = details.get("mapped_count", 0)
            unmapped = details.get("unmapped_count", 0)
            rate = details.get("mapping_rate", "0%")

            sections.append(
                f"{mapped:,} of {initial:,} rows ({rate}) were successfully mapped to the GBD cause list.\n"
            )
            sections.append(
                f"- **Mapped:** {mapped:,} rows\n"
                f"- **Unmapped:** {unmapped:,} rows\n"
            )

        # Document mapping sources
        source_types = {}
        for entry in mapping_entries:
            if entry.action in ["start_mapping", "mapping_complete", "review_file_generated"]:
                continue

            source_type = entry.action.split("_")[0]  # "direct", "fuzzy", "ai"
            if source_type not in source_types:
                source_types[source_type] = []

            source_types[source_type].append(entry)

        if source_types:
            sections.append("\n### Mapping Sources\n\n")

            for source_type, entries in source_types.items():
                sections.append(f"#### {source_type.title()} Mapping\n\n")

                for entry in entries:
                    details = entry.details
                    file_used = entry.file_used
                    mapped_count = entry.rows_affected or 0

                    if file_used:
                        sections.append(
                            f"- **Mapping File:** {file_used}\n"
                        )

                    if "version" in details:
                        sections.append(f"- **Version:** {details['version']}\n")

                    if "threshold" in details:
                        sections.append(
                            f"- **Confidence Threshold:** {details['threshold']}\n"
                        )

                    sections.append(f"- **Rows Mapped:** {mapped_count:,}\n\n")

        # Check for review file
        review_entry = next(
            (e for e in mapping_entries if e.action == "review_file_generated"), None
        )
        if review_entry:
            details = review_entry.details
            codes_count = details.get("unmapped_codes_count", 0)
            sections.append(
                f"### Human Review Required\n\n"
                f"**{codes_count}** unique unmapped codes were flagged for human review. "
                f"A file containing AI-suggested mappings (`human_review_required.csv`) has been "
                f"generated for expert validation.\n"
            )

        return "\n".join(sections)

    def _generate_quality_section(
        self,
        quality_results: Optional[Dict[str, Any]],
        provenance: ProvenanceTracker,
    ) -> str:
        """Generate quality assessment section."""
        if not quality_results:
            return "## Data Quality Assessment\n\nNo quality checks were performed."

        sections = ["## Data Quality Assessment\n"]

        quality_score = quality_results.get("quality_score", "N/A")
        checks_run = quality_results.get("checks_run", [])
        issues = quality_results.get("issues_found", [])

        sections.append(f"**Overall Quality Score:** {quality_score:.1f}/100\n\n")

        sections.append(f"**Checks Performed:** {len(checks_run)}\n")
        for check in checks_run:
            sections.append(f"- {check.replace('_', ' ').title()}\n")

        if issues:
            sections.append(f"\n**Issues Found:** {len(issues)}\n\n")

            # Group by severity
            errors = [i for i in issues if i.get("severity") == "error"]
            warnings = [i for i in issues if i.get("severity") == "warning"]

            if errors:
                sections.append("### Errors\n\n")
                for issue in errors:
                    sections.append(f"- **{issue.get('check', 'Unknown')}**: {issue.get('message', '')}\n")

            if warnings:
                sections.append("\n### Warnings\n\n")
                for issue in warnings[:10]:  # Limit to first 10 warnings
                    sections.append(f"- **{issue.get('check', 'Unknown')}**: {issue.get('message', '')}\n")

                if len(warnings) > 10:
                    sections.append(f"\n*... and {len(warnings) - 10} more warnings. See provenance log for details.*\n")
        else:
            sections.append("\n**No quality issues detected.**\n")

        return "\n".join(sections)

    def _generate_output_section(
        self, data: pd.DataFrame, provenance: ProvenanceTracker
    ) -> str:
        """Generate final output section."""
        io_entries = [e for e in provenance.entries if e.step == "io"]
        save_entry = next((e for e in io_entries if e.action == "data_saved"), None)

        output_file = "Not specified"
        if save_entry:
            output_file = save_entry.details.get("output_file", "Not specified")

        return f"""## Final Output

The harmonized dataset contains **{len(data):,} rows** and **{len(data.columns)} columns**.

**Output File:** {output_file}

**Columns in Final Dataset:**
{self._format_column_list(list(data.columns))}
"""

    def _generate_appendices(self, provenance: ProvenanceTracker) -> str:
        """Generate appendices section."""
        return f"""## Appendices

### A. Provenance Log

A complete provenance log documenting every transformation applied during this harmonization run is available in `provenance.json`.

**Run ID:** {provenance.run_id}  
**Total Entries:** {len(provenance.entries)}

### B. Reproducibility

This harmonization process is fully reproducible. To reproduce these results:

1. Use the same input data file
2. Use the same `config.yaml` configuration file
3. Run the AutoGBD pipeline with the same version

The provenance log provides a complete audit trail of all transformations applied.

### C. Framework Information

- **Framework:** AutoGBD v0.1.0
- **Processing Date:** {provenance.start_time.strftime('%Y-%m-%d %H:%M:%S')}
- **Total Processing Time:** {provenance.to_dict().get('duration_seconds', 0):.2f} seconds

---

*This report was automatically generated by the AutoGBD harmonization framework.*
"""

